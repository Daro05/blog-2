{
  
    
        "post0": {
            "title": "Blogging for Data Scientists: Moving Away from Medium",
            "content": "I recently began using fastpages, a blogging platform based on GitHub Pages and Jekyll, that makes it easier to write technical blog posts using Jupyter notebooks. Over the years, I’d become accustomed to the familiarity and simplicity of Medium. While migrating to a self-hosted service can be quite intimidating for someone used to off-the-shelf solutions, I decided to go ahead and do it anyway (once I learned how simple it was to do using fastpages). I also began thinking about what it is that makes me blog in the first place. And the more I thought about it, the more I realized Medium wasn’t the right platform for me any more. If you’re a Data Scientist, or anyone in computing who likes writing blog posts involving code snippets, seriously, please, reconsider using Medium as your default option. . What’s wrong with Medium? . Okay, so you already have a Medium account, and you’ve been happily publishing posts for months without any problems. But if you just step back for a moment and think about it, you’ll see there are a number of problems. . Free isn’t free . As a data scientist and otherwise computer-savvy person, you’ll have spent countless hours on stackoverflow and other blogs, all of which provided you that knowledge for free. No strings attached (alright, some of them did come with ads, I’ll admit). On the other hand, Medium’s exasperating drive to promote starred posts that are behind a paywall, has proven it’s doing the exact opposite. Your posts will be pushed to the top of its recommendation list if it has the most click-baity, contrarian title, commonly used tech jargon, and of course, the famous ⭐ icon. If most Data Science and computing-related posts on Medium were written by people who acquired their knowledge (largely) through free portals, why should Medium be a gatekeeper to spreading more knowledge in this space? . Note, I say this having been a paid subscriber to Medium for more than two years. As time went on, I found myself getting less and less value from my subscription. Yes, I’m aware that one can view paywalled Medium articles for free in incognito mode, but the sheer inconvenience of having to bypass a paywall each time I need to take a quick glance at possibly useful information is a huge disincentive on its own. Not to mention the annoyance of having to see that “subscribe to Medium” message every time I open a post (I know, because I’ve been there, done that and and still not changed the world)! . Poor user experience with code-formatting . Because Medium wasn’t written from the ground up to support code-filled blog posts (including inline code-formatting was a mere afterthought) - this leads to a lot of Medium posts being written with just bland code snippets, without syntax-highlighting. This places a significant burden on the writer to convert individual blocks of code to GitHub gists and embedded into a Medium post for a decent-looking snippet that can be read and understood easily. Most writers of posts choose not to use gists (due to the inconvenience factor, or simply because they are unaware), instead publishing code snippets using the inbuilt code formatter without syntax-highlighting, making it a poor user experience from both the reader’s and writer’s perspective. . Non-existent support for mathematical symbols . You’re limited to posting ugly images of mathematical symbols and equations on Medium. While this may look okay at the time of writing your post, it’s almost guaranteed to lose quality once it’s rendered onto Medium’s template and viewed on a host of different devices (try loading a math-heavy Medium post on a device with dark-mode enabled). Quality aside, it’s generally considered poor practice to post mathematical symbols as images (for accessibility reasons). . Reliance on a single platform for content . As Medium’s user base has grown, more and more people are writing content for the top publications (in Data Science and machine learning), which is not in itself a bad thing. However, encouraging users to subscribe to a publication and providing a “one-stop-shop” for all things Data Science/ML-related means that users are pigeon-holed into obtaining their information from a single website. In the long run, great information is self-selected by a decentralized system (such as Google’s search algorithm), so it’s highly likely that your post will show up at the top of a relevant Google search anyway. . Note: From my experience writing on Medium for Data Science-related topics, most traffic in the longer term comes through Google searches and external referrals, so why not just publish high-quality posts with insightful, relevant content on your own blog instead? . fastpages - what does it offer? . For those of you who are intimidated by building your own blog site and hosting it (for free), fastpages (built by Jeremy Howard and Hamel Husain) lets you do all of this from within the comfort of GitHub. Because it’s based on GitHub Pages - a free service that lets you build and host your own website, it doesn’t come with any hidden baggage (all you need is a GitHub account, which you’ll already have, or will consider getting if you’re coding on a regular basis). . Jeremy provides a summary in his Tweet thread (with the linked blog post inside): . twitter: https://twitter.com/jeremyphoward/status/1232059428238581760?s=20 . But in addition to those points, here are a few more reasons why you should consider using your own blog site. . Pre-built user interface . A large part of the hassle of building your own blog site is designing and implementing your own UI. With fastpages, all the initial heavy lifting is done for you - a more than decent UI is provided by default, which contains all the essential components that one might ask for in a personal blog site: . Ability to use a personal domain name (myname.dev) instead of myname.github.io/blogname | Cleanly organized posts with image previews | An “About” section describing who you are and your background | Tags for searching blog posts by topic | Badges that link visitors to your GitHub/Twitter pages | . Ready-made code formatting and syntax-highlighting . Posts in fastpages are written in directly in a Jupyter notebook, or using regular markdown, using a feature-rich editor (like VS Code). Depending on the language you’re highlighting, it’s a walk in the park to include code snippets that look exactly as you intended them to. . Support for LaTeX-style equations . fastpages goes the extra mile and allows you to type in LaTeX-style mathematical symbols and equations, rendering them on your blog for you. This works with inline as well as stand-alone equations. . Example inline equation: $ alpha + beta = gamma$ . Keeps code and the related blog all in one place . Because Jupyter notebooks are essentially self-documenting blog posts themselves, fastpages turbo-charges their use by offering custom syntax hints to directly publish a Jupyter notebook as a blog post (using an extension of the nbdev project). This is extremely useful, because it essentially reduces the labour of describing your Data Science workflow and experiments by allowing you to document your thought process along with the code itself. This is a radical shift from the way you’ll have been conventionally blogging about such workflows (e.g. via Medium), where you’ll have had multiple browser tabs open for copy-pasting code, GitHub gists and more. . Additional thoughts: My own experience blogging with notebooks . Personally, this is one aspect of using fastpages that has changed the way I think about blogging. Earlier, writing a blog post was always a significant upfront planning effort on my part - I’d lay out snippets of code that I’d need to publish in the blog beforehand. Then, I’d painstakingly create GitHub gists for each snippet so that I could embed them into my Medium post in between parts of the workflow where I was trying to explain a point. With notebooks, the whole process is organic, because you’re testing code snippets on the fly along with the text for your blog side-by-side! . Even when using pure Markdown to write a blog post (as I did with this one), it’s as simple as working in a text editor and using nbdev syntax to render effects such as highlighted tips and code folding (as shown in the original blog post). . Examples of customizing fastpages . Change syntax-highlight colours . Change the default font-size . Adding the Disqus comment system . FAQs . What about viewership? Will I get significantly fewer readers? . Won’t my productivity be affected if I have to make a commit each time? . If you’re writing blog posts about data-manipulation workflows with code snippets, it’s very likely you’re already committing the code somewhere. Also, if you’re looking to increase your credibility in the field by publishing insightful, thought-out software development workflows, committing your blog to GitHub can only bring you more brownie points (by having an active commit history). . Can I save drafts of my post before actually publishing it? . Conclusions . Why do you write blog posts in the first place? . I’ve learned firsthand that writing about complex technical workflows cements those concepts in my brain. The more you do it, the better you become at conceptualizing those workflows for new, unseen situations. Using a service like Medium offers you the wrong reward functions - that is, to increase viewership and receive more “claps”. Again, from my firsthand experience, having a decent number of claps on a Medium post did nothing to enhance my portfolio (after a point, receiving a clap meant nothing). Real jobs in the real world come from networking and talking to the right people, and a large part of that is how you communicate your thought process. What better way to do so than getting your ideas out there in your own blog, without the pressure of virtual “pats on the back” from an online system! . Note: Online abuse and toxicity is a very legitimate concern, so disabling comments on your posts using fastpages or any other service is a valid, viable option for many. . The goal of writing a technical blog post, in my view, should be to provide at least some useful insight to the reader. Even if it just benefits five people in the entire world, that’s a worthy reward in itself - because it is knowledge that was likely gained through a free and open portal (such as someone else’s blog), and is shared with someone else through another free portal (this also aligns well with the overall goals of the world wide web). . The key to writing good posts is to find a reward function that works for you. . Where to from here? . In the long run, you’re not tied to hosting on GitHub and using fastpages - you can save the intermediate Markdown generated in the backend, so you can always migrate to another service if you choose to. | As you upskill, you’ll find better ways to add new features, colours and functionality to your blog, and eventually, you’ll not be limited to relying on someone else’s template. | .",
            "url": "https://prrao87.github.io/blog/blogging-for-data-scientists/",
            "relUrl": "/blogging-for-data-scientists/",
            "date": " • May 2, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Turbo-charge your spaCy NLP pipeline",
            "content": ". Background . Consider you have a large tabular dataset on which you want to apply some non-trivial NLP transformations, such as stopword removal followed by lemmatizing (i.e. reducing to root form) the words in a text. spaCy is an industrial strength NLP library designed for just such a task. . In the example shown below, the New York Times Kaggle dataset is used to showcase how to significantly speed up a spaCy NLP pipeline. the goal is to take in an article&#39;s text, and speedily return a list of lemmas with unnecessary words, called &quot;stopwords&quot;, removed. . Pandas DataFrames provide a convenient interface to work with tabular data of this nature. First, import the necessary modules shown. . #collapse-hide import re import pandas as pd import spacy . . Initial steps . The news data is obtained by running the preprocessing notebook ./data/preprocessing.ipynb, which processes the raw text file downloaded from Kaggle and performs some basic cleaning on it. This step generates a file that contains the tabular data (stored as nytimes.tsv). A curated stopword file is also provided in ./data/stopwords. . Additionally, during initial testing, we can limit the size of the DataFrame being worked on (to a subset of the total number of articles) for faster execution. For the final run, disable the limit by setting it to zero. . #collapse-hide inputfile = &quot;data/spacy_multiprocess/nytimes-sample.tsv&quot; stopwordfile = &quot;data/spacy_multiprocess/stopwords/stopwords.txt&quot; limit = 0 . . Load spaCy model . For lemmatization, a simple spaCy model can be initialized. Since we will not be doing any specialized tasks such as dependency parsing and named entity recognition in this exercise, these components are disabled. . . Tip: spaCy has a sentencizer component that can be plugged into a blank pipeline. . The sentencizer pipeline simply performs tokenization and sentence boundary detection, following which lemmas can be extracted as token properties. . nlp = spacy.load(&#39;en_core_web_sm&#39;, disable=[&#39;tagger&#39;, &#39;parser&#39;, &#39;ner&#39;]) nlp.add_pipe(nlp.create_pipe(&#39;sentencizer&#39;)) . A method is defined to read in stopwords from a text file and convert it to a set in Python (for efficient lookup). . def get_stopwords(): &quot;Return a set of stopwords read in from a file.&quot; with open(stopwordfile) as f: stopwords = [] for line in f: stopwords.append(line.strip(&quot; n&quot;)) # Convert to set for performance stopwords_set = set(stopwords) return stopwords_set stopwords = get_stopwords() . Read in New York Times Dataset . The pre-processed version of the NYT news dataset is read in as a Pandas DataFrame. The columns are named date, headline and content - the text present in the content column is what will be preprocessed to remove stopwords and generate token lemmas. . def read_data(inputfile): &quot;Read in a tab-separated file with date, headline and news content&quot; df = pd.read_csv(inputfile, sep=&#39; t&#39;, header=None, names=[&#39;date&#39;, &#39;headline&#39;, &#39;content&#39;]) df[&#39;date&#39;] = pd.to_datetime(df[&#39;date&#39;], format=&quot;%Y-%m-%d&quot;) return df . df = read_data(inputfile) df.head(3) . date headline content . 0 2016-06-30 | washington nationals max scherzer baffles mets... | Stellar pitching kept the Mets afloat in the f... | . 1 2016-06-30 | mayor de blasios counsel to leave next month t... | Mayor Bill de Blasio’s counsel and chief legal... | . 2 2016-06-30 | three men charged in killing of cuomo administ... | In the early morning hours of Labor Day last y... | . Define text cleaner . Since the news article data comes from a raw HTML dump, it is very messy and contains a host of unnecessary symbols, social media handles, URLs and other artifacts. An easy way to clean it up is to use a regex that parses only alphanumeric strings and hyphens (so as to include hyphenated words) that are between a given length (3 and 50). This filters each document down to only meaningful text for the lemmatizer. . def cleaner(df): &quot;Extract relevant text from DataFrame using a regex&quot; # Regex pattern for only alphanumeric, hyphenated text with 3 or more chars pattern = re.compile(r&quot;[A-Za-z0-9 -]{3,50}&quot;) df[&#39;clean&#39;] = df[&#39;content&#39;].str.findall(pattern).str.join(&#39; &#39;) if limit &gt; 0: return df.iloc[:limit, :].copy() else: return df . df_preproc = cleaner(df) df_preproc.head(3) . date headline content clean . 0 2016-06-30 | washington nationals max scherzer baffles mets... | Stellar pitching kept the Mets afloat in the f... | Stellar pitching kept the Mets afloat the firs... | . 1 2016-06-30 | mayor de blasios counsel to leave next month t... | Mayor Bill de Blasio’s counsel and chief legal... | Mayor Bill Blasio counsel and chief legal advi... | . 2 2016-06-30 | three men charged in killing of cuomo administ... | In the early morning hours of Labor Day last y... | the early morning hours Labor Day last year gr... | . Now that we have just the clean, alphanumeric tokens left over, these can be further cleaned up by removing stopwords before proceeding to lemmatization. . Option 1: Work directly on the data using pandas.Series.apply . The straightforward way to process this text is to use an existing method, in this case the lemmatize method shown below, and apply it to the clean column of the DataFrame. Lemmatization is done using the spaCy&#39;s underlying Doc representation of each token, which contains a lemma_ property. Stopwords are removed simultaneously with the lemmatization process, as each of these steps involves iterating through the same list of tokens. . def lemmatize(text): &quot;&quot;&quot;Perform lemmatization and stopword removal in the clean text Returns a list of lemmas &quot;&quot;&quot; doc = nlp(text) lemma_list = [str(tok.lemma_).lower() for tok in doc if tok.is_alpha and tok.text.lower() not in stopwords] return lemma_list . The resulting lemmas as stored as a list in a separate column preproc as shown below. . %%time df_preproc[&#39;preproc&#39;] = df_preproc[&#39;clean&#39;].apply(lemmatize) df_preproc[[&#39;date&#39;, &#39;content&#39;, &#39;preproc&#39;]].head(3) . CPU times: user 48.5 s, sys: 146 ms, total: 48.6 s Wall time: 48.6 s . date content preproc . 0 2016-06-30 | Stellar pitching kept the Mets afloat in the f... | [stellar, pitch, keep, mets, afloat, half, sea... | . 1 2016-06-30 | Mayor Bill de Blasio’s counsel and chief legal... | [mayor, bill, blasio, counsel, chief, legal, a... | . 2 2016-06-30 | In the early morning hours of Labor Day last y... | [early, labor, group, gunman, street, gang, cr... | . Applying this method to the clean column of the DataFrame and timing it shows that it takes almost a minute to run on 8,800 news articles. . Option 2: Use nlp.pipe . Can we do better? in the spaCy documentation, it is stated that &quot;processing texts as a stream is usually more efficient than processing them one-by-one&quot;. This is done by calling a language pipe, which internally divides the data into batches to reduce the number of pure-Python function calls. This means that the larger the data, the better the performance gain that can be obtained by nlp.pipe. . To use the language pipe to stream texts, a separate lemmatizer method is defined that directly works on a spaCy Doc object. This method is then called in batches to work on a sequence of Doc objects that are streamed through the pipe as shown below. . def lemmatize_pipe(doc): lemma_list = [str(tok.lemma_).lower() for tok in doc if tok.is_alpha and tok.text.lower() not in stopwords] return lemma_list def preprocess_pipe(texts): preproc_pipe = [] for doc in nlp.pipe(texts, batch_size=20): preproc_pipe.append(lemmatize_pipe(doc)) return preproc_pipe . Just as before, a new column is created by passing data from the clean column of the existing DataFrame. Note that unlike in workflow #1 above, we do not use the apply method here - instead, the column of data (an iterable) is directly passed as an argument to the preprocessor pipe method. . %%time df_preproc[&#39;preproc_pipe&#39;] = preprocess_pipe(df_preproc[&#39;clean&#39;]) df_preproc[[&#39;date&#39;, &#39;content&#39;, &#39;preproc_pipe&#39;]].head(3) . CPU times: user 51.6 s, sys: 144 ms, total: 51.8 s Wall time: 51.8 s . date content preproc_pipe . 0 2016-06-30 | Stellar pitching kept the Mets afloat in the f... | [stellar, pitch, keep, mets, afloat, half, sea... | . 1 2016-06-30 | Mayor Bill de Blasio’s counsel and chief legal... | [mayor, bill, blasio, counsel, chief, legal, a... | . 2 2016-06-30 | In the early morning hours of Labor Day last y... | [early, labor, group, gunman, street, gang, cr... | . Timing this workflow doesn&#39;t seem to show improvement over the previous workflow, but as per the spaCy documentation, one would expect that as we work on bigger and bigger datasets, this approach would show some timing improvement (on average). . Option 3: Parallelize the work using joblib . We can do still better! The previous workflows sequentially worked through each news document to produce the lemma lists, which were then appended to the DataFrame as a new column. Because each row&#39;s output is completely independent of the other, this is an embarassingly parallel problem, making it ideal for using multiple cores. . The joblib library is recommended by spaCy for processing blocks of an NLP pipeline in parallel. Make sure that you pip install joblib before running the below section. . To parallelize the workflow, a few more helper methods must be defined. . Chunking: The news article content is a list of (long) strings where each document represents a single article&#39;s text. This data must be fed in &quot;chunks&quot; to each worker process started by joblib. Each call of the chunker method returns a generator that only contains that particular chunk&#39;s text as a list of strings. During lemmatization, each new chunk is retrieved based on the iterator index (with the previous chunks being &quot;forgotten&quot;). | . Flattening: Once joblib creates a set of worker processes that work on each chunk, each worker returns a &quot;list of list&quot; containing lemmas for each document. These lists are then combined by the executor to provide a deeply nested final &quot;list of list of lists&quot;. To ensure that the length of the output from the executor is the same as the actual number of articles, a &quot;flatten&quot; method is defined to combine the result into a list of lists containing lemmas. For example, if the executor returns a final result [[[a, b, c], [d, e, f]], [[g, h, i], [j, k, l]]], a flattened version of this result would be [[a, b, c], [d, e, f], [g, h, i], [j, k, l]]. | . In addition to the above methods, a similar nlp.pipe method is used as in workflow #2, on each chunk of texts. Each of these methods is wrapped into a preprocess_parallel method that defines the number of worker processes to be used (7 in this case), breaks the input data into chunks and returns a flattened result that can then be appended to the DataFrame. For machine with a higher number of physical cores, the number of worker processes can be increased further. . from joblib import Parallel, delayed from functools import partial def chunker(iterable, total_length, chunksize): return (iterable[pos: pos + chunksize] for pos in range(0, total_length, chunksize)) def flatten(list_of_lists): &quot;Flatten a list of lists to a combined list&quot; return [item for sublist in list_of_lists for item in sublist] def process_chunk(texts): preproc_pipe = [] for doc in nlp.pipe(texts, batch_size=20): preproc_pipe.append(lemmatize_pipe(doc)) return preproc_pipe def preprocess_parallel(texts, chunksize=100): executor = Parallel(n_jobs=7, backend=&#39;multiprocessing&#39;, prefer=&quot;processes&quot;) do = delayed(process_chunk) tasks = (do(chunk) for chunk in chunker(texts, len(df_preproc), chunksize=chunksize)) result = executor(tasks) return flatten(result) . %%time df_preproc[&#39;preproc_parallel&#39;] = preprocess_parallel(df_preproc[&#39;clean&#39;], chunksize=1000) . CPU times: user 683 ms, sys: 248 ms, total: 932 ms Wall time: 17.2 s . df_preproc[[&#39;date&#39;, &#39;content&#39;, &#39;preproc_parallel&#39;]].head(3) . date content preproc_parallel . 0 2016-06-30 | Stellar pitching kept the Mets afloat in the f... | [stellar, pitch, keep, mets, afloat, half, sea... | . 1 2016-06-30 | Mayor Bill de Blasio’s counsel and chief legal... | [mayor, bill, blasio, counsel, chief, legal, a... | . 2 2016-06-30 | In the early morning hours of Labor Day last y... | [early, labor, group, gunman, street, gang, cr... | . Timing this parallelized workflow shows significant performance gains (almost 3x reduction in run time)! As the number of documents becomes larger, the additional overhead of starting multiple worker threads with joblib is quickly paid for, and this method can significantly outperform the sequential methods. . Effect of chunk size and batch size . Note that in the parallelized workflow, two parameters need to be specified - the optimum number can vary depending on the dataset. The chunksize controls the number of chunks being worked on by each process. In this example, for 8,800 documents, a chunksize of 1000 is used. Too small a chunksize would mean that a large number of worker threads would spawn (each one waiting for other threads to complete), which can slow down execution. Generally, a chunksize of 10% the total number of documents can be used as a starting point (assuming that all chunks fit into memory at any given time). . The batch size is parameter specific to nlp.pipe, and again, a good value depends on the data being worked on. For reasonably long-sized text such as news articles, it makes sense to keep the batch size reasonably small (so that each batch doesn&#39;t contain really long texts), so in this case 20 was chosen for the batch size. For other cases (e.g. Tweets) where each document is much shorter in length, a larger batch size can be used. . It is recommended to experiment with either parameter to see which combination produces the best performance. . Additional gains . . Important: Use sets over lists for lookups wherever possible. . Note that in the get_stopwords() method defined earlier on, the list of stopwords read in from the stopword file was converted to a set before using it in the lemmatizer method for stopword removal via lookups. This is a very useful trick in general, but specifically for stopword removal, the use of sets becomes all the more important. Why? . In any realistic stopword list, such as this one for a news dataset, it&#39;s reasonable to expect several hundred stopwords. This is because for downstream tasks such as topic modelling or sentiment analysis, there are a number of domain-specific words that need to be removed (very common verbs, useless abbreviations such as timezones, days of the week, etc.). Each word in each and every document needs to be compared against every word in the stopword list, which is an expensive operation over tens of thousands of documents. . It&#39;s well known that sets have $O(1)$ (i.e. constant) lookup time as opposed to lists, which have $O(n)$ lookup time. In the lemmatize() method, since we&#39;re checking each word for membership in the set of stopwords, we would expect sets to be much better than lists. To test this, we can rerun workflow #1 but this time, use a stopword list instead. . stopwords = list(stopwords) . %%time df_preproc[&#39;preproc_stopword_list&#39;] = df_preproc[&#39;clean&#39;].apply(lemmatize) df_preproc[[&#39;date&#39;, &#39;content&#39;, &#39;preproc_stopword_list&#39;]].head(3) . CPU times: user 1min 17s, sys: 108 ms, total: 1min 18s Wall time: 1min 18s . date content preproc_stopword_list . 0 2016-06-30 | Stellar pitching kept the Mets afloat in the f... | [stellar, pitch, keep, mets, afloat, half, sea... | . 1 2016-06-30 | Mayor Bill de Blasio’s counsel and chief legal... | [mayor, bill, blasio, counsel, chief, legal, a... | . 2 2016-06-30 | In the early morning hours of Labor Day last y... | [early, labor, group, gunman, street, gang, cr... | . This method now takes ~50% longer than it did before (when using a stopword set), which is almost a 1.5x increase in run time! This makes sense because in this case, the stopword list is about 500 words long, and each and every word in the corpus needs to be checked for membership in this reasonable-sized list. . Conclusions . In this exercise, a news article dataset (NY Times) was processed using a spaCy pipeline to output a list of lemmas representing the useful tokens present in each article&#39;s content. Because real-world news datasets are almost certainly bigger than this one, and can be almost unbounded in size, a fast, efficient NLP pipeline is necessary to perform any meaningful analysis on the data. The following steps are very useful in speeding up the spaCy pipeline. . Disable unnecessary components in spaCy model: The standard spaCy model&#39;s pipeline contains the tagger (to assign part-of-speech tags), the parser (to generate a dependency parse) and named entity recognition components. If any or none of these actions are desired, these components must be disabled immediately after loading the model (as shown above). . Use sets over lists for lookups: When performing lookups to compare one set of tokens against another, always check for membership using sets - lists are significantly slower for lookups! The larger the list/set of stopwords, the larger the performance gain seen when using sets. . Use custom language pipes when possible: Setting up a language pipe using nlp.pipe is an extremely flexible and efficient way to process large blocks of text. Even better, spaCy allows you to individually disable components for each specific sub-task, for example, when we need to separately perform part-of-speech tagging and named entity recognition (NER). See the spaCy docs for examples on how to disable pipeline components during model loading, processing or handling custom blocks. . Use multiple cores when possible: When processing individual documents completely independent of one another, consider parallelizing the workflow by passing the computation to multiple cores. As the number of documents becoms higher and higher, the performance gains can be tremendous. One just needs to ensure that the documents are divided up into chunks, all of which must fit into memory at any given time. . Hope this was useful and have fun testing these out in your next NLP project! .",
            "url": "https://prrao87.github.io/blog/spacy/nlp/performance/2020/04/29/spacy-multiprocess.html",
            "relUrl": "/spacy/nlp/performance/2020/04/29/spacy-multiprocess.html",
            "date": " • Apr 29, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://prrao87.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://prrao87.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}