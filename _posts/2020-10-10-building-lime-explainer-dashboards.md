---
title: "Building a LIME explainer dashboard in Python"
description: "A comparison of Flask, Plotly Dash and Streamlit to build dashboards that provide LIME explanations for classification results"
toc: false
comments: true
layout: post
image: "images/posts/doors.jpg"
author: Prashanth Rao
categories: [nlp, visualization]
hide: true
search_exclude: false
permalink: /lime-explainer-dashboards/
---

![](../images/posts/doors.jpg)

# Background

In [an earlier post](https://towardsdatascience.com/fine-grained-sentiment-analysis-in-python-part-2-2a92fdc0160d), I described how to explain a fine-grained sentiment classifier's results using LIME (**L**ocal **I**nterpretable **M**odel-agnostic **E**xplanations). To recap, the following six models were used to make fine-grained sentiment class predictions on the Stanford Sentiment Treebank (SST-5) dataset.

* Rule-based models: **TextBlob** and **VADER**
* Feature-based models: **Logistic regression** and **Support Vector Machine**
* Embedding-based models: **FastText** and **Flair**

A linear workflow was used to analyze and explain the sentiment classification results using each method. Each model was trained on 5 classes of sentiment (1 through 5), with 1 being "strongly negative", 3 being "neutral" and 5 being "strongly positive".


![](lime_explainer/sentiment-workflow.png "Sentiment classification: Training & evaluation pipeline")

The goal of this post is to show how to build an explainer dashboard (using any one of three frameworks) that takes in a trained model, and outputs LIME explanations for the prediction made by the model.

## Example LIME explanation
Put simply, LIME generates an explanation object containing visualizations (as embedded JavaScript) that can be output to an HTML file, which can then be opened in any browser. A typical output from LIME is shown below.

![](lime_explainer/example_lime.png "Example LIME explanation visualized via HTML")

## Why build an interactive dashboard app?
To explain a classifier's results using LIME, it can be very cumbersome to have to write out individual HTML files each time an explanation needs to be made. An interactive dashboard is a very effective means to rapidly iterate through multiple test samples in real time, providing a user with immediate feedback on where a model is lacking. In addition, having a dashboard allows non-technical users (who may or may not know how to execute Python scripts) to be able to make their own explanations.

The below sections show how we can build LIME explainer dashboards using three different frameworks.

# Explainer class
To easily reference each classifier's predict methods, an object-oriented framework is applied to enable code reuse. A template for the explainer class is provided in [the project's GitHub repo](https://github.com/prrao87/fine-grained-sentiment/blob/master/explainer.py). In a nutshell, a Python class is defined which takes in the list of variations generated by LIME (random text samples with tokens blanked out), following which we output a class probability for each sample as a numpy array.

```python
class ExampleExplainer:
    """Class to explain classification results.
       The `predict` method outputs a numpy array of floats, which
       is the classifier's prediction probability for each class.
    """
    def __init__(self, path_to_model: str) -> None:
        # Load in a trained classifier model
        
    def predict(self, texts: List[str]) -> np.array([float, ...]):
        # Take in a list of strings (LIME text sample of variations)
        ...
        # Output class probabilities as a numpy array
        return np.array(predict_proba)
```

Once the class probabilities for each variation is returned, this can be fed to the `LimeTextExplainer` class (shown below). Enabling bag-of-words (`bow`) would mean that LIME doesn’t consider word order when generating variations. However, the FastText and Flair models were trained considering n-grams and contextual ordering respectively, so for a fair comparison between models, the `bow` flag option is disabled for all explanations on SST-5.

The `exp` object returned by the LIME explainer is an internal method that converts the local linear model’s predictions (in numerical form) to a visual, interpretable form, output as HTML.

```python
def explainer(
        method: str,
        path_to_file: str,
        text: str,
        num_samples: int) -> LimeTextExplainer:
    """Run LIME explainer on provided classifier"""
    model = explainer_class(method, path_to_file)
    predictor = model.predict

    # Create a LimeTextExplainer
    explainer = LimeTextExplainer(
        # Specify split option for string
        split_expression=lambda x: x.split(),
        # Our classifer uses N-grams or contextual ordering to classify text
        # Hence, order matters, and we cannot use bag of words.
        bow=False,
        # Specify class names for this case
        class_names=[1, 2, 3, 4, 5]
    )

    # Make a prediction and explain it:
    exp = explainer.explain_instance(
        text,
        classifier_fn=predictor,
        top_labels=1,
        num_features=20,
        num_samples=num_samples,
    )
    return exp
```

The below sections describe how to encapsulate all these functions into an interactive dashboard app.

# Option 1: Flask
