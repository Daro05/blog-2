---
title: "Building a LIME explainer dashboard in Python"
description: "A comparison of Flask, Plotly Dash and Streamlit to build dashboards that provide LIME explanations for classification results"
toc: false
comments: true
layout: post
image: "images/posts/doors.jpg"
author: Prashanth Rao
categories: [nlp, dataviz]
hide: true
search_exclude: false
permalink: /lime-explainer-dashboards/
---

![](../images/posts/doors.jpg)

# Background

In [an earlier post](https://towardsdatascience.com/fine-grained-sentiment-analysis-in-python-part-2-2a92fdc0160d), I described how to explain a fine-grained sentiment classifier's results using LIME (**L**ocal **I**nterpretable **M**odel-agnostic **E**xplanations). To recap, the following six models were used to make fine-grained sentiment class predictions on the Stanford Sentiment Treebank (SST-5) dataset.

* Rule-based models: **TextBlob** and **VADER**
* Feature-based models: **Logistic regression** and **Support Vector Machine**
* Embedding-based models: **FastText** and **Flair**

A linear workflow was used to analyze and explain the sentiment classification results using each method. Each model was trained on 5 classes of sentiment (1 through 5), with 1 being "strongly negative", 3 being "neutral" and 5 being "strongly positive".


![](lime_explainer/sentiment-workflow.png "Sentiment classification: Training & evaluation pipeline")

The goal of this post is to show how to build an explainer dashboard (using any one of three frameworks) that takes in a trained model, and outputs LIME explanations for the prediction made by the model.

## Example LIME explanation
Put simply, LIME generates an explanation object containing visualizations (as embedded JavaScript) that can be output to an HTML file, which can then be opened in any browser. A typical output from LIME is shown below.

![](lime_explainer/example_lime.png "Example LIME explanation visualized via HTML")

## Why build an interactive dashboard app?
To explain a classifier's results using LIME, it can be cumbersome to have to write out individual HTML files each time an explanation needs to be made. An interactive dashboard that takes in user input is a very effective means to rapidly iterate through multiple test samples in real time, providing the user with immediate feedback. In addition, having a dashboard allows non-technical users (who may or may not know how to execute Python scripts) to be able to make their own LIME explanations on demand.

The below sections show how we can build LIME explainer dashboards using three different frameworks.

# Explainer class
To easily reference each classifier's predict methods, the below object-oriented template is applied to enable code reuse, available in[the project's GitHub repo](https://github.com/prrao87/fine-grained-sentiment/blob/master/explainer.py). In a nutshell, a Python class is defined which takes in the list of variations generated by LIME (random text samples with tokens blanked out), following which we output a class probability for each sample as a numpy array.

```python
class ExampleExplainer:
    """Class to explain classification results.
       The `predict` method outputs a numpy array of floats, which
       is the classifier's prediction probability for each class.
    """
    def __init__(self, path_to_model: str) -> None:
        # Load in a trained classifier model

    def predict(self, texts: List[str]) -> np.array([float, ...]):
        # Take in a list of strings (LIME text sample of variations)
        ...
        # Output class probabilities as a numpy array
        return np.array(predict_proba)
```

Once the class probabilities for each variation is returned, this can be fed to the `LimeTextExplainer` class (shown below). Enabling bag-of-words (`bow`) would mean that LIME doesn’t consider word order when generating variations. However, the FastText and Flair models were trained considering n-grams and contextual ordering respectively, so for a fair comparison between models, the `bow` flag option is disabled for all explanations on SST-5.

The `exp` object returned by the LIME explainer is via the `explain_instance` method internal to LIME, that converts the local linear model’s predictions (in numerical form) to a visual, interpretable form. This can then be output as HTML.

```python
def explainer(
        method: str,
        path_to_file: str,
        text: str,
        num_samples: int) -> LimeTextExplainer:
    """Run LIME explainer on provided classifier"""
    model = explainer_class(method, path_to_file)
    predictor = model.predict

    # Create a LimeTextExplainer
    explainer = LimeTextExplainer(
        # Specify split option for string
        split_expression=lambda x: x.split(),
        # Our classifer uses N-grams or contextual ordering to classify text
        # Hence, order matters, and we cannot use bag of words.
        bow=False,
        # Specify class names for this case
        class_names=[1, 2, 3, 4, 5]
    )

    # Make a prediction and explain it:
    exp = explainer.explain_instance(
        text,
        classifier_fn=predictor,
        top_labels=1,
        num_features=20,
        num_samples=num_samples,
    )
    return exp
```

The below sections describe how to encapsulate all these functions into an interactive dashboard app.

# Option 1: Flask

A Flask version of the LIME explainer dashboard is shown below. The user enters a piece of text, selects the number of random samples to generate for LIME, and then chooses from a given list of classifiers using the dropdown menu. Clicking on the `Explain results!` button then generates a LIME explanation HTML object, which is rendered within an HTML IFrame.

![](lime_explainer/explainer-app-flask.gif "Flask version of LIME explainer")

Although Flask isn't a data dashboarding tool (it is a [WSGI](https://wsgi.readthedocs.io/) web framework that began as a wrapper around [Werkzeug](https://werkzeug.palletsprojects.com/) and [Jinja](https://palletsprojects.com/p/jinja/)), it provides a simple plugin-based architecture from which developers can build interfaces to complex applications. The key strength of Flask is its robustness in a production setting and the numerous extensions around it in the Python ecosystem.

To build a LIME explainer dashboard using Flask, it is required to know about the following pieces of technology:

* __HTML/JavaScript__: A page's overall structure and content is defined using HTML. Any actions that need to be triggered based on a field's value or user input needs to be defined using JavaScript, either through the HTML file itself, or loaded from an external source.
* __CSS__: A page's styles and layout are defined using a CSS file.
* __Jinja2__: This is a templating engine that generates the page's HTML dynamically from within Python. It is necessary because of security reasons (not using templates and passing around unescaped, static HTML can result in cross-site scripting attacks). The template engine is controlled from within Python, and the final HTML is rendered using Flask methods.

## Flask app: directory structure
The directory structure used for the Flask app is shown below. First, the required styles are configured using CSS in the `static/style.css` file, and the HTML template to be rendered is defined in `templates/index.html`. Any trained sentiment classifier models go into the `models` directory. The explainer class is defined in `lime_explainer.py` and the Flask routes in `app.py`.

```
.
├── models
|   ├── classifier1
|   └── classifier2
├── static
|   └── style.css
└── templates
|   └── index.html
├── app.py
└── lime_explainer.py
```

The application code in Flask is written in the simplest way possible. Two routes are defined (the default route `'/'` and the route for the LIME results, `'result'`). Note that the results route uses a `POST` request, meaning that it only generates HTML (via Jinja) once the user inputs some information to the app and interacts with it.

```python
import os
from flask import Flask, request, render_template
from lime_explainer import explainer, tokenizer, METHODS

app = Flask(__name__)
SECRET_KEY = os.urandom(24)

@app.route('/')
@app.route('/result', methods=['POST'])
def index():
    exp = ""
    if request.method == 'POST':
        text = tokenizer(request.form['entry'])
        method = request.form['classifier']
        n_samples = request.form['n_samples']
        if any(not v for v in [text, n_samples]):
            raise ValueError("Please do not leave text fields blank.")

        if method != "base":
            exp = explainer(method,
                            path_to_file=METHODS[method]['file'],
                            text=text,
                            lowercase=METHODS[method]['lowercase'],
                            num_samples=int(n_samples))
            exp = exp.as_html()

        return render_template('index.html', exp=exp, entry=text, n_samples=n_samples, classifier=method)
    return render_template('index.html', exp=exp)


if __name__ == '__main__':
    app.secret_key = SECRET_KEY
    app.run(debug=True)
```

The code for the Flask app is [available here on GitHub](https://github.com/prrao87/fine-grained-sentiment-app).


# Option 2: Dash (by Plotly)



# Option 3: Streamlit


# Deployment


# A note on scalability in web frameworks
WSGI vs. asynchronous event loops.

# When does it make the most sense to use each framework?

# Conclusions